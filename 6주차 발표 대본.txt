LSTM은 ‘Long Short Term Memory라고 하는 장단기 메모리로, 앞서 설명한 RNN의 일종이라고 볼 수 있습니다.
    
LSTM은 RNN의 기울기 소멸 문제와 장기 의존성 문제를 해결 및 보완하기 위해 고안된 기법입니다.  
    
이에 대해 설명하기 위해 활성화함수에 대해 말씀드리겠습니다.
    
 우선 활성화함수는 **입력 신호의 총합을 출력 신호로 변환하는 함수를 말하는데, LSTM은 활성화함수 중 시그모이드 함수와 tanh 함수와 연관이 깊습니다.
    
시그모이드란 ‘S자 모양’이라는 뜻으로, 실수 값을 입력 받아 0에서 1사이의 값으로 압축하여 표현한 함수입니다. 큰 음수 값일수록 0에 가까워지고 큰 양수 값일수록 1이 됩니다.
    
이런 시그모이드 함수에는 단점이 있는데 그것이 RNN의 문제와도 같은 기울기 소멸 문제가 발생한다는 것입니다. 
    
시그모이드 함수의 기울기는 역전파 중에 이전의 기울기와 현재 기울기를 곱하면서 점점 기울기가 사라지게 됩니다. 이렇게 되면 신경망의 학습 능력이 제한되게 됩니다. 
    
Tanh함수는 시그모이드 함수와 비슷하지만 실수 값을 입력받아 -1에서 1 사이의 값으로 함수를 압축합니다.  Tanh 함수는 시그모이드에 비해 최적화를 잘하지만 시그모이드 함수와 마찬가지로 기울기 소멸 문제를 가지고 있습니다. 
    
이렇게 기울기 소멸이 일어나기 때문에 RNN은 장기 의존성에 대한 문제를 가지고 있습니다. 
    
RNN은 바로 직전의 정보 뿐만 아니라 더 이전의 정보를 기억하고 있고, 망각에 대한 기능이 없다보니 단어의 가중치가 수십단계가 아닌 100단계 이상이 된다면 계산량이 너무 커져 기억할 수 없게 됩니다. 계속 가중하다 기울기가 소실되면서 기울기가 폭발하게 되면 기억을 할 수 없기 때문에 RNN은 단기기억만을 이용하게 됩니다.
앞서 설명한 RNN의 문제점을 보완하고자 한 것이 바로 LSTM입니다.
    
LSTM은 ‘Memory Cell’을 도입하여 셀의 정보를 어떤 것을 기억하고 잊을지 결정할 수 있게 보완된 알고리즘입니다.
    
RNN에서는 이전 셀의 기억(정보전달)이 하나였던 반면, LSTM에서는 출력 외에 기억이 추가되어 2개의 라인으로 되어있습니다. LSTM은 단기와 장기를 연관시키면서 각각 다른 라인에서 기억을 보존하고 있다고 보면 됩니다.
    
이전 셀의 출력, 즉 단기 기억은 지금 현재 입력하는 셀과 합류하여 4개의 라인에 분기(동일 정보 복사)됩니다. 이 단계는 RNN과 같습니다. 분기되는 4개의 라인은 각각 망각 게이트, input게이트,셀의 상태를 바꿔주는 cell state update 게이트. output게이트입니다.
다음으로 가장 윗 라인인 망각 게이트에서는 이전 셀에서의 장기 기억 하나하나에 대해 0~1 사이의 값으로 정보의 취사 선택을 하는 것입니다. 1은 뭐든 남기고, 0은 전부 버립니다. 단기기억과 입력으로 인식한 시점에서 장기 기억속의 내용이 중요하지 않다고 판단했을 때, 시그모이드 함수의 출력은 0 근처의 값이 되어 이 기억을 망각하고, 중요하다고 느낀 정보는 1로 그대로 남아있습니다. LSTM은 RNN과 다르게 망각 게이트에 의해 원치 않는 정보를 버림으로써 폭발을 방지하는 것입니다.
    
이렇게 동작하는 것이 LSTM 알고리즘 입니다.